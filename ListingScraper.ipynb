{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "import requests \n",
    "import pandas as pd \n",
    "import schedule \n",
    "import time \n",
    "import datetime  \n",
    "from pathlib import Path \n",
    "import googlemaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION WILL FETCH THE DATA FROM THE URL AND PASS IT TO BEAUTIFULSOUP \n",
    "def connection_zoopla():\n",
    "    size = 25 # Amount of properties per request that. It will be used in query string(url).\n",
    "    area = 'Edinburgh' # City of 'Edinburgh' is assigned to area. A different city could be introduced here. It will be used in Query string(url).\n",
    "    URL = ('https://www.zoopla.co.uk/for-sale/property/%s/?q=%s&search_source=home&pn=1&view_type=list&page_size=%d'\n",
    "           %(area, area,size))# Zoopla URL is given from where the data has to be scraped. This could be modified to add new cities.\n",
    "    r = requests.get(URL) # It will sent the request to the server of given url.\n",
    "    soup = BeautifulSoup(r.content, 'html5lib') # BeautifulSoup will parse the requests object to HTML5.\n",
    "    return soup # Returning soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will filter required data from BeautifulSoup and put it in a dictionary.\n",
    "def get_data_zoopla(soup):\n",
    "    \n",
    "    # This is the main class where all the properties are .\n",
    "    table = soup.find('ul', attrs = {'class':'listing-results clearfix js-gtm-list'}) #'ul' is the tag () \n",
    "    \n",
    "    price_final = [] \n",
    "    bed_final = [] \n",
    "    date_final = [] \n",
    "    address_final = [] \n",
    "    agent_final = [] \n",
    "    \n",
    "    # ITERATE TO GET ALL THE PROPERTIES IN THE URL PAGE, AND APPEND TO THE LISTS\n",
    "    for row in table.findAll('li', attrs = {'class': 'srp clearfix'}): \n",
    "\n",
    "        price = row.find('a', attrs = {'class': 'listing-results-price text-price'}).text # Fetching the 'price' value in each loop.\n",
    "        price = price.replace(' ', '').replace('Â£', '').replace(',', '') # Data cleaning: Replace unnecessary space, sign and comma\n",
    "        price = price.split() # Convert price string to list\n",
    "        price = price[0] # Take very first, means 0th element \n",
    "        price_final.append(price) # Appending the price in the list.\n",
    "\n",
    "        bed = row.find('h2', attrs = {'class': 'listing-results-attr'}).text # Fetching the 'bed' value in each loop.\n",
    "        bed = bed.split() # Replace unnecessary space, sign and comma\n",
    "        bed = bed[0] # Take very first, means 0th element \n",
    "        bed_final.append(bed) # Appending the bed in the list.\n",
    "\n",
    "        date_agent = row.find('p', attrs = {'class': 'top-half listing-results-marketed'}).text # Fetching the 'date and agent' data in each loop.\n",
    "        date_agent = date_agent.split() # Replace unnecessary space, sign and comma\n",
    "        \n",
    "        date = [date_agent[2].replace('th', ''), date_agent[3], date_agent[4]] # Get date, month and year in the list.\n",
    "        date = '-'.join(map(str, date)) # List to string which is join by '-'.\n",
    "        date = datetime.datetime.strptime(date, '%d-%b-%Y') # Convert date string into date object.\n",
    "        date = date.strftime(\"%d-%m-%Y\") # Fetch appropriate format of date within the date object.\n",
    "        date_final.append(date) # Appending the date in the list.\n",
    "        \n",
    "        agent =' '.join(date_agent[6:])\n",
    "        agent_final.append(agent)\n",
    "        \n",
    "        address = row.find('a', attrs = {'class': 'listing-results-address'}).text # Fetching the 'address' value in each loop.\n",
    "        address_final.append(address) # Appending the address in the list.\n",
    "        \n",
    "    \n",
    "    data_dictionary = {'Address': address_final, 'Asking Price': price_final, \n",
    "                       'Date Posted': date_final, 'Number of beds': bed_final, 'Estate Agent':agent_final\n",
    "                      } \n",
    "    \n",
    "    return data_dictionary # Return data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines a DataFrame by:\n",
    "    # 1- Adding columns from the dictionary created previously\n",
    "    # 2- Adding two additional columns \"Address_input\" that is used later to search for the sold price and 'Sold Price' which is populated later\n",
    "    # 3- Adding two fields(LAN, LON) from the address.\n",
    "    # 4- Removes incomplete addresses which dont have house number and can not be tracked later (the code uses the fact that all the addresses should start with a number, i.e. 5/3 Hartingon... or 66 Hillside...)\n",
    "    \n",
    "def create_dataframe(data_dictionary):\n",
    "    df = pd.DataFrame(data=data_dictionary) # Create DataFrame from dictionary.\n",
    "    df['Date Posted'] = pd.to_datetime(df['Date Posted']) # String to datetime.\n",
    "    df['Asking Price'] = pd.to_numeric(df['Asking Price']) # String to numeric.\n",
    "    \n",
    "    df['Address_input'] = df['Address'].str.replace(' ','+').str.replace('/','%2F').str.replace(',','%2C').str.replace('(','%28').str.replace(')','%29').str.replace('\"','%22')\n",
    "    \n",
    "    df[\"LAT\"] = None # Define null column.\n",
    "    df[\"LON\"] = None # Define null column.\n",
    "    \n",
    "    \n",
    "    df = df[df['Address'].astype(str).str.startswith(('1','2','3','4','5','6','7','8','9'))] # removes incomplete addresses which don't start with a number\n",
    "    \n",
    "    # THIS SECTION IS COMMENTED OUT AS A VALID GOOGLEKEY NEEDS TO BE USED.\n",
    "    #google_key=\n",
    "    #gmaps = googlemaps.Client(key = google_key)\n",
    "    for i in range(0, len(df)): # It is used to add each latitude and longitude based on the addresses.\n",
    "        \n",
    "        geocode_result = gmaps.geocode(df.iat[i,0]) # Get address.\n",
    "        try: \n",
    "            lat = geocode_result[0][\"geometry\"][\"location\"][\"lat\"] \n",
    "            lon = geocode_result[0][\"geometry\"][\"location\"][\"lng\"] \n",
    "            df.iat[i, df.columns.get_loc(\"LAT\")] = lat \n",
    "            df.iat[i, df.columns.get_loc(\"LON\")] = lon \n",
    "        except:\n",
    "            lat = None\n",
    "            lon = None\n",
    "            \n",
    "    df['Sold Price'] = None\n",
    "    df[['Address', 'Asking Price', 'Date Posted', 'Number of beds', 'LAT','LON', 'Sold Price', 'Address_input', 'Estate Agent']]\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to create CSV file from DataFrame.\n",
    "def create_file(df):\n",
    "    \n",
    "    file_name = 'Edinburgh_Final'\n",
    "    path = Path(file_name)\n",
    "    \n",
    "    if path.is_file(): # check if file exits.\n",
    "        old_data = pd.read_csv(file_name)  # Get previous data into the dataframe from the CSV file.\n",
    "        new_data = pd.concat([old_data, df]) # Concatenate previous DataFrame and new DataFrame.\n",
    "        new_data  = new_data.drop_duplicates(subset='Address', keep=\"first\", inplace=False) # Delete duplicate column.\n",
    "    else:\n",
    "        new_data = df # Assign new DataFrame.\n",
    "\n",
    "    with open(file_name, 'w+', newline='') as f: # Create new file.\n",
    "        new_data.to_csv(f, index=False) # Create CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to call all the above functions in order, to scrape the data and create dataframe.\n",
    "def final_scraper():\n",
    "    soup = connection_zoopla()\n",
    "    data_dictionary = get_data(soup)\n",
    "    df = create_dataframe(data_dictionary)\n",
    "    create_file(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scheduler to automate the function on every day\n",
    "\n",
    "schedule.every(1).day.at(\"23:00\").do(final_scraper)\n",
    "#schedule.every(1).minutes.do(final_scraper)                                 \n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
